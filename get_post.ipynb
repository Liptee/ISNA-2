{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log import *\n",
    "import vk_api\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим посты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getter(group_id):\n",
    "  vk_session = vk_api.VkApi(login, psswd)\n",
    "  try:\n",
    "      vk_session.auth(token_only=True)\n",
    "  except vk_api.AuthError as error_msg:\n",
    "      print(error_msg)\n",
    "      return\n",
    "  tools = vk_api.VkTools(vk_session)\n",
    "  wall = tools.get_all('wall.get', 25,{'owner_id': group_id}, limit=200)\n",
    "  print('Posts count:', wall['count'])\n",
    "  wall = wall[\"items\"]\n",
    "  for row in wall:\n",
    "    with open(r\"wall_{}.csv\".format(group_id), \"a\", encoding=\"utf-8\") as f:\n",
    "      f.write(str(row))\n",
    "      f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter(-69812)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем и предобрабатываем собранный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wall_-43938013.csv\", on_bad_lines='skip', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = df[7]\n",
    "posts = list(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(posts):\n",
    "    res = []\n",
    "    for post in posts:\n",
    "        p = post.split(\"'text':\")[1]\n",
    "        p = p[2:]\n",
    "        res.append(p)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = extract(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for row in posts:\n",
    "    all_text += row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = all_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abv = \"йцукенгшщзхъфывапролджэячсмитьбюё \"\n",
    "abc = []\n",
    "for i in abv:\n",
    "    abc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_text = \"\"\n",
    "for i in range(len(all_text)):\n",
    "    if all_text[i] in abc:\n",
    "        clear_text+=all_text[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clear_text.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Произведем лемматизацию текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for idx in range(len(tokens)):\n",
    "    tokens[idx] = morph.parse(tokens[idx])[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_text = \"\"\n",
    "for token in tokens:\n",
    "    lemma_text+=token\n",
    "    lemma_text+=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lemma_text.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(lemma_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проведем стемминг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(clear_text)\n",
    "for idx in range(len(tokens)):\n",
    "    tokens[idx] = stemmer.stem(tokens[idx])\n",
    "stem_text = \"\"\n",
    "for token in tokens:\n",
    "    stem_text+=token\n",
    "    stem_text+=\" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stem_text.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(stem_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('Lab_2-zARI9a1x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907a74e250388877b60673f81d9745fd136c55194997f2bc3ee1d81b60a1c990"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
